{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的,我們可以將梯度下降的概念延伸到反向傳播神經網絡(BPN)。以下是BPN的數學表示和簡單的Python實現:\n",
    "\n",
    "1. 數學表示:\n",
    "\n",
    "假設我們有一個簡單的三層神經網絡(輸入層、隱藏層、輸出層):\n",
    "\n",
    "- 輸入: x = [x₁, x₂, ..., xₙ]\n",
    "- 隱藏層權重: W₁, 偏置: b₁\n",
    "- 輸出層權重: W₂, 偏置: b₂\n",
    "- 激活函數: σ (例如sigmoid函數)\n",
    "\n",
    "## 前向傳播:\n",
    "## z₁ = W₁x + b₁\n",
    "## a₁ = σ(z₁)\n",
    "## z₂ = W₂a₁ + b₂\n",
    "## y = σ(z₂)\n",
    "\n",
    "## 反向傳播:\n",
    "## δ₂ = (y - t) ⊙ σ'(z₂)  (t是目標輸出)\n",
    "其中:\n",
    "- δ₂: 輸出層的誤差\n",
    "- y: 實際輸出\n",
    "- t: 目標輸出\n",
    "- ⊙: 哈達瑪積（元素間逐點相乘）\n",
    "- σ': sigmoid函數的導數\n",
    "- z₂: 輸出層的加權和\n",
    "\n",
    "## δ₁ = (W₂ᵀδ₂) ⊙ σ'(z₁)\n",
    "\n",
    "其中:\n",
    "- δ₁: 隱藏層的誤差\n",
    "- W₂ᵀ: W₂的轉置矩陣\n",
    "- z₁: 隱藏層的加權和\n",
    "\n",
    "## 更新規則:\n",
    "## W₂ = W₂ - α * δ₂a₁ᵀ\n",
    "## b₂ = b₂ - α * δ₂\n",
    "## W₁ = W₁ - α * δ₁xᵀ\n",
    "## b₁ = b₁ - α * δ₁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Python實現 BPN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01379359]\n",
      " [0.97906213]\n",
      " [0.97993797]\n",
      " [0.02263262]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # 導入 NumPy 庫，用於進行數值計算和數組操作\n",
    "\n",
    "# 定義 sigmoid 激活函數\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  # 計算並返回 sigmoid 函數值\n",
    "\n",
    "# 定義 sigmoid 函數的導數\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # 計算並返回 sigmoid 函數的導數\n",
    "\n",
    "# 定義神經網絡類\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x  # 設置輸入數據\n",
    "        self.weights1 = np.random.rand(self.input.shape[1], 4)  # 初始化第一層權重：使用numpy的random.rand函數生成一個隨機矩陣，大小為輸入特徵數量乘以4（隱藏層神經元數量）\n",
    "        self.weights2 = np.random.rand(4, 1)  # 初始化第二層權重：生成一個4行1列的隨機矩陣，連接隱藏層到輸出層\n",
    "        self.y = y  # 設置目標輸出：將傳入的目標值y賦給self.y，用於後續的訓練和誤差計算\n",
    "        self.output = np.zeros(y.shape)  # 初始化輸出為零矩陣：創建一個與目標輸出y相同形狀的零矩陣，用於存儲網絡的預測結果\n",
    "\n",
    "    # 前向傳播\n",
    "    def feedforward(self):\n",
    "        # 前向傳播函數\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))  # 計算隱藏層的輸出\n",
    "        # 詳細解釋：\n",
    "        # 1. np.dot(self.input, self.weights1)：將輸入數據與第一層權重相乘，進行矩陣乘法運算\n",
    "        # 2. sigmoid()：將上述矩陣乘法的結果通過sigmoid激活函數，將值壓縮到0到1之間\n",
    "        # 3. self.layer1：將sigmoid函數的結果存儲為隱藏層的輸出，供後續計算使用\n",
    "\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))  # 計算輸出層的輸出\n",
    "        # 詳細解釋：\n",
    "        # 1. np.dot(self.layer1, self.weights2)：將隱藏層的輸出與第二層權重相乘，進行矩陣乘法運算\n",
    "        # 2. sigmoid()：將上述矩陣乘法的結果通過sigmoid激活函數，將值壓縮到0到1之間\n",
    "        # 3. self.output：將sigmoid函數的結果存儲為神經網絡的最終輸出，這就是網絡的預測結果\n",
    "\n",
    "    # 反向傳播函數\n",
    "    def backprop(self):\n",
    "        # 計算輸出層的誤差和權重調整\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        # 詳細解釋：\n",
    "        # 1. (self.y - self.output)：計算實際輸出與預期輸出之間的差異\n",
    "        # 2. sigmoid_derivative(self.output)：計算輸出層的sigmoid函數導數\n",
    "        # 3. 2*(self.y - self.output) * sigmoid_derivative(self.output)：計算輸出層的誤差項\n",
    "        # 4. np.dot(self.layer1.T, ...)：將隱藏層的輸出與輸出層的誤差項進行矩陣乘法，得到第二層權重的調整量\n",
    "\n",
    "        # 計算隱藏層的誤差和權重調整\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "        # 詳細解釋：\n",
    "        # 1. 2*(self.y - self.output) * sigmoid_derivative(self.output)：計算輸出層的誤差項\n",
    "        # 2. np.dot(..., self.weights2.T)：將輸出層的誤差反向傳播到隱藏層\n",
    "        # 3. sigmoid_derivative(self.layer1)：計算隱藏層的sigmoid函數導數\n",
    "        # 4. ... * sigmoid_derivative(self.layer1)：計算隱藏層的誤差項\n",
    "        # 5. np.dot(self.input.T, ...)：將輸入數據與隱藏層的誤差項進行矩陣乘法，得到第一層權重的調整量\n",
    "\n",
    "        # 更新權重\n",
    "        self.weights1 += d_weights1  # 更新第一層權重：將計算得到的調整量加到原權重上\n",
    "        self.weights2 += d_weights2  # 更新第二層權重：將計算得到的調整量加到原權重上\n",
    "\n",
    "    # 訓練網絡函數\n",
    "    def train(self, iterations):\n",
    "        for _ in range(iterations):  # 迭代指定的次數，進行多次訓練\n",
    "            self.feedforward()  # 執行前向傳播，計算網絡輸出\n",
    "            self.backprop()  # 執行反向傳播，更新網絡權重\n",
    "\n",
    "# 使用示例\n",
    "X = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])  # 定義輸入數據：4個樣本，每個樣本有3個特徵\n",
    "y = np.array([[0], [1], [1], [0]])  # 定義目標輸出：對應的4個樣本的期望輸出\n",
    "\n",
    "nn = NeuralNetwork(X, y)  # 創建神經網絡實例，使用給定的輸入數據和目標輸出初始化網絡\n",
    "nn.train(1500)  # 訓練網絡 1500 次迭代，通過反覆的前向傳播和反向傳播來優化網絡權重\n",
    "\n",
    "print(nn.output)  # 輸出最終結果：顯示經過訓練後網絡對輸入數據的預測結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失計算：\n",
    "1. 隱含的損失計算：\n",
    "   雖然這個範例中沒有明確計算和輸出損失值，但損失的概念實際上是隱含在反向傳播過程中的。在 `backprop` 方法中，`(self.y - self.output)` 這部分實際上代表了預測值與真實值之間的差異，這就是損失的一種形式。\n",
    "\n",
    "2. 簡化的實現：\n",
    "   這個範例是一個簡化的神經網絡實現，主要目的是展示反向傳播的基本原理。為了保持代碼簡潔，省略了顯式的損失計算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 改進\n",
    "為了使這個範例更完整，我們可以添加一個損失計算和輸出。這不僅能幫助我們監控訓練進度，還能判斷模型是否收斂。\n",
    "\n",
    "以下是如何修改代碼以包含損失計算的建議：\n",
    "\n",
    "```python:d:\\Dev_prjs\\AI_MIT\\113-1_deep_mit\\W06\\sup02_back_progration_nerual_network.ipynb\n",
    "class NeuralNetwork:\n",
    "    # ... 其他代碼保持不變 ...\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        # 使用均方誤差（MSE）作為損失函數\n",
    "        return np.mean(np.square(self.y - self.output))\n",
    "\n",
    "    def train(self, iterations):\n",
    "        for i in range(iterations):\n",
    "            self.feedforward()\n",
    "            self.backprop()\n",
    "            \n",
    "            # 每100次迭代輸出一次損失\n",
    "            if i % 100 == 0:\n",
    "                loss = self.calculate_loss()\n",
    "                print(f\"迭代 {i}: 損失 = {loss}\")\n",
    "\n",
    "# 使用示例\n",
    "X = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork(X, y)\n",
    "nn.train(1500)\n",
    "\n",
    "# 訓練結束後輸出最終損失\n",
    "final_loss = nn.calculate_loss()\n",
    "print(f\"最終損失: {final_loss}\")\n",
    "print(\"最終輸出:\")\n",
    "print(nn.output)\n",
    "```\n",
    "\n",
    "這個修改版本添加了以下內容：\n",
    "1. `calculate_loss` 方法用於計算均方誤差（MSE）。\n",
    "2. 在 `train` 方法中，每100次迭代輸出一次當前的損失值。\n",
    "3. 訓練結束後，輸出最終的損失值和網絡輸出。\n",
    "\n",
    "通過這些修改，我們可以更好地追蹤訓練過程，觀察損失值的變化，從而判斷模型的訓練效果和收斂情況。這對於理解和改進神經網絡的訓練過程非常有幫助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # 導入 NumPy 庫，用於進行數值計算和數組操作\n",
    "\n",
    "# 定義 sigmoid 激活函數\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  # 計算並返回 sigmoid 函數值\n",
    "\n",
    "# 定義 sigmoid 函數的導數\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # 計算並返回 sigmoid 函數的導數\n",
    "\n",
    "# 定義神經網絡類\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x  # 設置輸入數據\n",
    "        self.weights1 = np.random.rand(self.input.shape[1], 4)  # 初始化第一層權重：使用numpy的random.rand函數生成一個隨機矩陣，大小為輸入特徵數量乘以4（隱藏層神經元數量）\n",
    "        self.weights2 = np.random.rand(4, 1)  # 初始化第二層權重：生成一個4行1列的隨機矩陣，連接隱藏層到輸出層\n",
    "        self.y = y  # 設置目標輸出：將傳入的目標值y賦給self.y，用於後續的訓練和誤差計算\n",
    "        self.output = np.zeros(y.shape)  # 初始化輸出為零矩陣：創建一個與目標輸出y相同形狀的零矩陣，用於存儲網絡的預測結果\n",
    "\n",
    "    # 前向傳播\n",
    "    def feedforward(self):\n",
    "        # 前向傳播函數\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))  # 計算隱藏層的輸出\n",
    "        # 詳細解釋：\n",
    "        # 1. np.dot(self.input, self.weights1)：將輸入數據與第一層權重相乘，進行矩陣乘法運算\n",
    "        # 2. sigmoid()：將上述矩陣乘法的結果通過sigmoid激活函數，將值壓縮到0到1之間\n",
    "        # 3. self.layer1：將sigmoid函數的結果存儲為隱藏層的輸出，供後續計算使用\n",
    "\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))  # 計算輸出層的輸出\n",
    "        # 詳細解釋：\n",
    "        # 1. np.dot(self.layer1, self.weights2)：將隱藏層的輸出與第二層權重相乘，進行矩陣乘法運算\n",
    "        # 2. sigmoid()：將上述矩陣乘法的結果通過sigmoid激活函數，將值壓縮到0到1之間\n",
    "        # 3. self.output：將sigmoid函數的結果存儲為神經網絡的最終輸出，這就是網絡的預測結果\n",
    "\n",
    "    # 反向傳播函數\n",
    "    def backprop(self):\n",
    "        # 計算輸出層的誤差和權重調整\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        # 詳細解釋：\n",
    "        # 1. (self.y - self.output)：計算實際輸出與預期輸出之間的差異\n",
    "        # 2. sigmoid_derivative(self.output)：計算輸出層的sigmoid函數導數\n",
    "        # 3. 2*(self.y - self.output) * sigmoid_derivative(self.output)：計算輸出層的誤差項\n",
    "        # 4. np.dot(self.layer1.T, ...)：將隱藏層的輸出與輸出層的誤差項進行矩陣乘法，得到第二層權重的調整量\n",
    "\n",
    "        # 計算隱藏層的誤差和權重調整\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "        # 詳細解釋：\n",
    "        # 1. 2*(self.y - self.output) * sigmoid_derivative(self.output)：計算輸出層的誤差項\n",
    "        # 2. np.dot(..., self.weights2.T)：將輸出層的誤差反向傳播到隱藏層\n",
    "        # 3. sigmoid_derivative(self.layer1)：計算隱藏層的sigmoid函數導數\n",
    "        # 4. ... * sigmoid_derivative(self.layer1)：計算隱藏層的誤差項\n",
    "        # 5. np.dot(self.input.T, ...)：將輸入數據與隱藏層的誤差項進行矩陣乘法，得到第一層權重的調整量\n",
    "\n",
    "        # 更新權重\n",
    "        self.weights1 += d_weights1  # 更新第一層權重：將計算得到的調整量加到原權重上\n",
    "        self.weights2 += d_weights2  # 更新第二層權重：將計算得到的調整量加到原權重上\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        # 使用均方誤差（MSE）作為損失函數\n",
    "        return np.mean(np.square(self.y - self.output))\n",
    "\n",
    "    def train(self, iterations):\n",
    "        for i in range(iterations):\n",
    "            self.feedforward()\n",
    "            self.backprop()\n",
    "            \n",
    "            # 每100次迭代輸出一次損失\n",
    "            if i % 100 == 0:\n",
    "                loss = self.calculate_loss()\n",
    "                print(f\"迭代 {i}: 損失 = {loss}\")\n",
    "                \n",
    "# 使用示例\n",
    "X = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork(X, y)\n",
    "nn.train(1500)\n",
    "\n",
    "# 訓練結束後輸出最終損失\n",
    "final_loss = nn.calculate_loss()\n",
    "print(f\"最終損失: {final_loss}\")\n",
    "print(\"最終輸出:\")\n",
    "print(nn.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
